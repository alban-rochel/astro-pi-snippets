from utils import *
import glob
import os
import json
import cv2
import numpy as np
from datetime import datetime,timedelta

"""
Our experiments aims at evaluating if the climate zone
as defined by Koppen (https://en.wikipedia.org/wiki/K%C3%B6ppen_climate_classification)
can be inferred by NDVI statistics.

When on dayside, we acquire an image, remove the porthole (mask),
remove the water (threshold on blue layer) and the clouds (threshold on grayscale),
compute NDVI, scaling it in the [0, 10] range of integers.

Thanks to a map of climates, we determine which climate we observe, and
we accumulate the measurements.

We are able to compute the mean and stdev of NDVI for every image AND every
climate zone.
We hope to be able to extract correlations, and maybe clusters of mean/stdev by zone.

Data is recorded so that we are able to replay the computations with other tunings
on the ground if needed.
"""

# Prepare the structure for registering the data used to compute statistics
data_store = dict()
# These are all our climate zone values
for climate_code in [0, 5, 10, 15, 20, 51, 52, 53, 101, 102, 151, 152, 153, 201, 202]:
    data_store[climate_code] = dict()
    for ndvi_score in range(11): # NDVI indexes will be scaled in the [0, 10] range
        data_store[climate_code][ndvi_score] = 0
    data_store[climate_code]["passes"] = 0

for folder in ["asterix1", "asterix2"]:
    
    porthole = cv2.imread(folder + "/porthole_mask.png", cv2.IMREAD_GRAYSCALE)
    
    for folder_file_name in glob.glob(folder+"/*image.jpg"):
        image_file_name = os.path.basename(folder_file_name)
        
        image = cv2.imread(folder_file_name, cv2.IMREAD_COLOR)
        
        json_file_name=image_file_name[:20]+"info.json"
        json_file = open(folder + "/" + json_file_name)
        json_data = json.load(json_file)
        
        time = json_data["time"]
        timestamp = time["year"] + time["month"] + time["day"] + "_" + time["hour"] + time["minute"] + time["second"] + time["micro"]
        
        climate_code = json_data["climate"]["code"]
        
        if not climate_code in [0, 5, 10, 15, 20, 51, 52, 53, 101, 102, 151, 152, 153, 201, 202]:
            climate_code = 0
            
        print(climate_code)
        
        # Compute where land is visible, on the cropped image
        # Get the clouds mask
        clouds = get_clouds_mask(image)
        
        # Get the land mask (= not water)
        land = get_land_mask(image)
        
        # Combine with the porthole mask
        visible_land_mask = cv2.bitwise_and(porthole, land)
        visible_land_mask = cv2.bitwise_and(visible_land_mask, cv2.bitwise_not(clouds))
        # Generate a timestamped name for the mask, and save it
        land_mask_file_name = generate_save_name("mask", timestamp, "png")
        cv2.imwrite(land_mask_file_name, visible_land_mask)
        
        # Compute NDVI indexes, ranging in [0, 10]
        ndvi = compute_ndvi(image)
        # Generate a timestamped name for the ndvi values, and save it
        ndvi_file_name = generate_save_name("ndvi", timestamp, "png")
        cv2.imwrite(ndvi_file_name, ndvi)
        
        # If on known climate zone, accumulate the number of points for each index
  
        rough_climate_code = int(climate_code/10) # int(climate_code/10) is the "rough-grain climate zone" (temperate, tropical...). If results are not exploitable in fine-grain zones, this may be better.
        json_data["ndvi"] = dict() # The scores will be saved with the image metadata (companion) json file
        for ndvi_index in range(11):
            # For each NDVI value, count the number of pixels on the visible land
            lower = np.array(ndvi_index, dtype = "uint8")
            upper = np.array(ndvi_index, dtype = "uint8")
            count = np.count_nonzero(cv2.bitwise_and(cv2.inRange(ndvi, lower, upper), visible_land_mask))
            json_data["ndvi"][ndvi_index] = count
            # Accumulate for the global computations
            if rough_climate_code in data_store:
                data_store[rough_climate_code][ndvi_index] += count
            if climate_code in data_store:
                data_store[climate_code][ndvi_index] += count
        data_store[climate_code]["passes"] += 1
        
        # Compute stats for the image
        stats = compute_stats(json_data["ndvi"])
        json_data["ndvi"]["mean"] = stats[0]
        json_data["ndvi"]["stdev"] = stats[1]
        
        # Save json companion file (metadata)
        metadata_file_name = generate_save_name("info", timestamp, "json")
        
        with open(metadata_file_name, "w") as outfile:
            json.dump(json_data, outfile, indent=2)

logger.info("Writing session statistics")
final_stats = dict()
for climate_code in [0, 5, 10, 15, 20, 51, 52, 53, 101, 102, 151, 152, 153, 201, 202]:
    final_stats[climate_code] = dict()
    final_stats[climate_code]["zone"] = climate_code_to_text(climate_code)
    stats = compute_stats(data_store[climate_code])
    final_stats[climate_code]["mean"] = stats[0]
    final_stats[climate_code]["stdev"] = stats[1]

# Save global results json file
final_file_name = generate_save_name("final", "final", "json")
with open(final_file_name, "w") as outfile:
    json.dump(final_stats, outfile, indent=2)
    
logger.info("Experiment done, thanks to the ESA and the Raspberry Pi Foundation!")